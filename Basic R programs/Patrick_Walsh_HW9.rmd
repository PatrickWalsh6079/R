# Intro to Data Science - HW 9
##### Copyright Jeffrey Stanton, Jeffrey Saltz, Christopher Dunham, and Jasmina Tacheva


```{r}
# Enter your name here: Patrick Walsh
```

### Attribution statement: (choose only one and delete the rest)


```{r}
# 1. I did this homework by myself, with help from the book and the professor.
```

**Text mining** plays an important role in many industries because of the prevalence of text in the interactions between customers and company representatives. Even when the customer interaction is by speech, rather than by chat or email, speech to text algorithms have gotten so good that transcriptions of these spoken word interactions are often available. To an increasing extent, a data scientist needs to be able to wield tools that turn a body of text into actionable insights. In this homework, we explore a real **City of Syracuse dataset** using the **quanteda** and **quanteda.textplots** packages. Make sure to install the **quanteda** and **quanteda.textplots** packages before following the steps below:<br>

## Part 1: Load and visualize the data file  
A.	Take a look at this article: https://samedelstein.medium.com/snowplow-naming-contest-data-2dcd38272caf and write a comment in your R script, briefly describing what it is about.<br>


```{r}
# The article is about residents of Syracuse voting on the name of a fleet of new snow plows for the city. There was
# some data analysis that had to go into picking the winning names, like for example, data cleaning by removing duplicate
# entries.
```

B.	Read the data from the following URL into a dataframe called **df**:
https://intro-datascience.s3.us-east-2.amazonaws.com/snowplownames.csv


```{r}
library(tidyverse)
df <- read_csv("https://intro-datascience.s3.us-east-2.amazonaws.com/snowplownames.csv")
glimpse(df)
```

C.	Inspect the **df** dataframe – which column contains an explanation of the meaning of each submitted snowplow name? Transform that column into a **document-feature matrix**, using the **corpus()**, **tokens()**, **tokens_select(), and **dfm()** functions. Do not forget to **remove stop words**.

Hint: Make sure you have libraried *quanteda*


```{r}
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)
# The column df$meaning contains an explanation of the meaning of the name.
dfCorpus <- corpus(df$meaning, docnames=df$submission_number)  # assigns df text data into a list
toks <- tokens(dfCorpus, remove_punct=TRUE)   # breaks sentences into tokens, removes punctuation
toks_nostop <- tokens_select(toks, pattern = stopwords("en"), selection = "remove")   # removes stopwords
dfDFM <- dfm(toks_nostop)  # convert corpus into document-feature matrix (DFM)
```

D.	Plot a **word cloud**, where a word is only represented if it appears **at least 2 times** . **Hint:** use **textplot_wordcloud()**:

Hint: Make sure you have libraried (and installed if needed) *quanteda.textplots* 


```{r}
textplot_wordcloud(dfDFM, min_count = 1)
```

E.	Next, **increase the minimum count to 10**. What happens to the word cloud? **Explain in a comment**. 


```{r}
textplot_wordcloud(dfDFM, min_count = 10)
# The word cloud only displays words that occur in the DFM 10 or more times, so there are less words displayed in the word cloud.
```

F.	What are the top words in the word cloud? Explain in a brief comment.


```{r}
# Looking at the word cloud, it looks like the top words are snow, syracuse, 1/2, and ï.
```

## Part 2: Analyze the sentiment of the descriptions

A.	Create a **named list of word counts by frequency**.<br>

output the 10 most frequent words (their word count and the word). <br>

**Hint**: use **textstat_frequency()** from the *quanteda.textstats* package.


```{r}
word_freq <- textstat_frequency(dfDFM)
top10 <- word_freq %>% arrange(-frequency)
top10[1:10,]
```

B.	Explain in a comment what you observed in the sorted list of word counts. 


```{r}
# The most common words, in order, are 1/2, ï, snow, and syracuse. This is in a different order than I thought from visually
# inspecting the word cloud.
```

## Part 3: Match the words with positive and negative words 

A.	Read in the list of positive words, using the scan() function, and output the first 5 words in the list. Do the same for the  the negative words list: <br>
<br>
https://intro-datascience.s3.us-east-2.amazonaws.com/positive-words.txt
<br>
https://intro-datascience.s3.us-east-2.amazonaws.com/negative-words.txt <br>
<br>

There should be 2006 positive words and 4783 negative words, so you may need to clean up these lists a bit. 


```{r}
posURL <- "https://intro-datascience.s3.us-east-2.amazonaws.com/positive-words.txt" 
posWords <- scan(posURL, character(0), sep = "\n") 
posWords <- posWords[-1:-34]
length(posWords)

negURL <- "https://intro-datascience.s3.us-east-2.amazonaws.com/negative-words.txt" 
negWords <- scan(negURL, character(0), sep = "\n") 
negWords <- negWords[-1:-34]
length(negWords)
```

B.	Use **dfm_match()** to match the words in the dfm with the words in posWords). Note that **dfm_match()** creates a new dfm.

Then pass this new dfm to the **textstat_frequency()** function to see the positive words in our corpus, and how many times each word was mentioned.


```{r}
posDFM <- dfm_match(dfDFM, posWords)  # Identifies positive words from the posDFM object inside the dfDFM object along with their index
posFreq <- textstat_frequency(posDFM)  # Counts how many of these positive words are found in the dfDFM object
glimpse(posFreq)
```

C. Sum all the positive words


```{r}
sum(posFreq$frequency)
```

D. Do a similar analysis for the negative words - show the 10 most frequent negative words and then sum the negative words in the document.


```{r}
negDFM <- dfm_match(dfDFM, negWords)  # Identifies negative words from the negDFM object inside the dfDFM object along with their index
negFreq <- textstat_frequency(negDFM)  # Counts how many of these negative words are found in the dfDFM object
glimpse(negFreq)
```

```{r}
top10_neg <- negFreq %>% arrange(-frequency)
top10_neg[1:10,]
```

```{r}
sum(negFreq$frequency)
```

E.	Write a comment describing what you found after matching positive and negative words. Which group is more common in this dataset? Might some of the negative words not actually be used in a negative way?  What about the positive words?
```{r}
# After a basic analysis of frequency count of positive and negative words, there are more positive words than negative words in the df.
# However, some of the negative words may be used in a positive way, depending on the context. For example, 'bad' could be negated by
# saying 'not bad'. Conversely, some of the positive words could be used in a negative way. For example, 'strong' could be used with having
# 'strong dislike' of the name.
```
